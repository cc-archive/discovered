#!/bin/bash
set -x
set -e # stop running script on any errors

./scripts/nuke-jena-store

ant

rm -rf crawl/

DISCOVER_ED_ROOT="."

# Get some command-line arguments.
if [ -n "$1" ]; then
    CURATOR_URI="$1"
else
    CURATOR_URI=http://a6.creativecommons.org/~raffi/sample_resources
fi

if [ -n "$2" ]; then
    FEED_URL="$2"
else
    FEED_URL=http://a6.creativecommons.org/~raffi/sample_resources/sample_rss.xml 
fi

# Add a curator
./bin/feeds addcurator "Sample resources" "$CURATOR_URI"

# Add a feed
./bin/feeds addfeed rss "$FEED_URL" "$CURATOR_URI"

# Check that all is well.
./bin/feeds listfeeds

# Do an aggregation
./bin/feeds aggregate

rm -rf seed/

mkdir seed

# Create a seed file that Nutch can use when it crawls the web
./bin/feeds seed > seed/urls.txt

echo "The seed file has this many lines:"
wc -l seed/urls.txt

./bin/nutch inject $DISCOVER_ED_ROOT/crawl/crawldb seed
./bin/nutch generate $DISCOVER_ED_ROOT/crawl/crawldb $DISCOVER_ED_ROOT/crawl/segments
./bin/nutch fetch $DISCOVER_ED_ROOT/crawl/segments/*
./bin/nutch updatedb $DISCOVER_ED_ROOT/crawl/crawldb $DISCOVER_ED_ROOT/crawl/segments/*
./bin/nutch invertlinks $DISCOVER_ED_ROOT/crawl/linkdb $DISCOVER_ED_ROOT/crawl/segments/*
./bin/nutch index $DISCOVER_ED_ROOT/crawl/indexes $DISCOVER_ED_ROOT/crawl/crawldb $DISCOVER_ED_ROOT/crawl/linkdb $DISCOVER_ED_ROOT/crawl/segments/*
